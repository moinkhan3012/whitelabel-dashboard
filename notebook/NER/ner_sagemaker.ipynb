{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b50d49b-2f93-41ad-ab40-4e54f1ff84fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a26cdbf-6d24-473f-97aa-901bf6014ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = sess.boto_session.region_name\n",
    "BUCKET_URI = \"s3://nerdatabucket\"\n",
    "BUCKET_NAME = \"nerdatabucket\"\n",
    "DATASET_PATH = f\"{BUCKET_URI}/amazon_product_tag_dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "317fc218-34e1-4ad2-b6ce-07f0717dfc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/train.py\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "def create_tokenize_data(tag_dataset_json):\n",
    "    tokenized_data = pd.DataFrame(columns=['tokens', 'ner_tags'])\n",
    "    for key, value in tag_dataset.items():\n",
    "        text = key\n",
    "        tokens = text.split()\n",
    "        labels = ['O'] * len(tokens)\n",
    "        for start, end, tag in value['entities']:\n",
    "            label_start_index = text[:start].count(\" \")\n",
    "            labels[label_start_index] = \"B-APP\"\n",
    "            #check if there are any spaces in the labelled app\n",
    "            spaces = text[start: end+1].count(\" \")\n",
    "            if spaces > 0:\n",
    "                #add Intermediate App label\n",
    "                for i in range(1, spaces+1):\n",
    "                    labels[label_start_index+i] = 'I-APP'\n",
    "        \n",
    "        tokenized_data = pd.concat([tokenized_data, pd.DataFrame([{'tokens': tokens, 'text': text, 'ner_tags': labels}])], ignore_index=True)\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "def tokenize_adjust_labels(all_samples_per_split, tokenizer):\n",
    "\n",
    "    total_adjusted_labels = []\n",
    "    label_names = {'O': 0, 'B-APP': 1, 'I-APP': 2}\n",
    "\n",
    "    tokenized_samples = tokenizer(all_samples_per_split[\"text\"])\n",
    "\n",
    "    word_ids_list = tokenized_samples.word_ids()\n",
    "    existing_label_ids = [-100] + [label_names[tag] for tag in all_samples_per_split[\"ner_tags\"]] + [-100]\n",
    "\n",
    "    tokenized_samples['labels'] = existing_label_ids\n",
    "\n",
    "    return pd.Series(tokenized_samples)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    label_names = {\n",
    "        0: 'O', 1:'B-APP', 2: 'I-APP'\n",
    "    }\n",
    "    print(p)\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    print(predictions)\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if(k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=16)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=16)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--model_name\", type=str)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--data_file\", type=str)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    # load json data\n",
    "    data_df = json.load(args.data_file)\n",
    "\n",
    "    # create tokenized data - converts JSON format to BIO format for NER\n",
    "    tokenized_data = create_tokenize_data(tag_dataset)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenized_data = tokenized_data.apply(lambda row: tokenize_adjust_labels(row, tokenizer), axis=1)\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    print(\"building training and testing datasets\")\n",
    "\n",
    "    # split dataset\n",
    "    train_df, test_df = train_test_split(tokenized_data, test_size=0.3, random_state=42)\n",
    "\n",
    "    # load dataset from pandas to HF\n",
    "    train_data = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    test_data = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "\n",
    "    # metric to monitor\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "    id2label= {\n",
    "        \"0\": \"LABEL_0\",\n",
    "        \"1\": \"LABEL_1\",\n",
    "        \"2\": \"LABEL_2\"\n",
    "    }\n",
    "\n",
    "    label2id= {\n",
    "        \"LABEL_0\": \"0\",\n",
    "        \"LABEL_1\": \"1\",\n",
    "        \"LABEL_2\": \"2\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    #initialized base model\n",
    "    model = AutoModelForTokenClassification.from_pretrained(args.model_name, id2label=id2label, label2id=label2id)\n",
    "\n",
    "\n",
    "    # set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.model_dir,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        learning_rate=float(args.learning_rate),\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    # initialized trainer job\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    # Persist model\n",
    "\n",
    "    # evaluate model\n",
    "    eval_result = trainer.evaluate(eval_dataset=test_data)\n",
    "\n",
    "    # writes eval result to file which can be accessed later in s3 ouput\n",
    "    with open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "        print(f\"***** Eval results *****\")\n",
    "        for key, value in sorted(eval_result.items()):\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    # Saves the model to s3\n",
    "    trainer.save_model(args.model_dir)\n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print(\"model persisted at \" + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1575b60-0e9b-460b-809e-2c2076c6b30b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "        'epochs': 3,\n",
    "        'train_batch_size': 16,\n",
    "        'eval_batch_size': 16,\n",
    "        'learning_rate': 5e-5,\n",
    "        'model_name': 'bert-base-uncased',\n",
    "        'data_file': DATASET_PATH,\n",
    "        'model_dir': BUCKET_URI\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb723a33-7e01-4835-877d-676d48c9cac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            use_spot_instances=True,\n",
    "                            role=role,\n",
    "                            transformers_version='4.26.0',\n",
    "                            pytorch_version='1.13.1',\n",
    "                            py_version='py39',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ce339-b9fe-4e1b-b99a-e782fe10c878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39061fce-b1fd-45be-b650-7e0071acd99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=f\"{BUCKET_URI}/model.tar.gz\",  \n",
    "   role=role, \n",
    "   transformers_version=\"4.26.0\", \n",
    "   pytorch_version=\"1.13\", \n",
    "   py_version=\"py39\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a82fe0-03f7-4fd9-a0c3-3603a5749701",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
