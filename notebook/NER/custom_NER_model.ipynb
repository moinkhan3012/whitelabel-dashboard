{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "pCDuU99Xxw_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import spacy\n",
        "import json"
      ],
      "metadata": {
        "id": "4RNe8YM5wJDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER Data Preparation"
      ],
      "metadata": {
        "id": "f8kDSW2j3J0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we convert the scrape data from csv to NER Json format i.e\n",
        "\n",
        "```\n",
        "{\n",
        "    \"sample_text\": {\n",
        "    \"entities\": [[Start_position, end_position, \"Entity_name\"]]\n",
        "    }\n",
        "}\n",
        "```\n",
        "eg.\n",
        "\n",
        "```\n",
        "{\n",
        "  \"useplug in open the Alexa app and get started in minutes\": {\n",
        "    \"entities\": [[20, 24, \"APP\"]]\n",
        "  },\n",
        "  \"and schedules through the Alexa app\": { \"entities\": [[26, 30, \"APP\"]] },\n",
        "  \"your smartphone using the Kasa app whether you are at home\": {\n",
        "    \"entities\": [[26, 29, \"APP\"]]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "Basic idea here is to manually label the NER dataset. We found out that most of the app name in the description is generally followed by \"app\" keyword. Follwing the same, for each product description, we split on \"app\" keyword and take 5 words from left and 5 words from right, and manually label them.\n",
        "\n"
      ],
      "metadata": {
        "id": "nGULUHvH3dWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Download the tokenizer models\n",
        "\n",
        "def get_all_contexts(text, target_word, context_size=5):\n",
        "    \"\"\"\n",
        "        Extract all contexts of a target word in a given text. Performs splitting operation on target words and considers context_size words on both sides of the target word.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text.\n",
        "            target_word (str): The target word for which contexts are to be extracted.\n",
        "            context_size (int): The number of words to consider on each side of the target word.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of strings representing all contexts of the target word.\n",
        "\n",
        "    \"\"\"\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "    # Find all occurrences of the target word\n",
        "    target_indices = [i for i, token in enumerate(tokens) if token.lower() == target_word.lower()]\n",
        "\n",
        "    # Extract context sentences for each occurrence of the target word\n",
        "    all_contexts = []\n",
        "    for target_index in target_indices:\n",
        "        start_index = max(0, target_index - context_size)\n",
        "        end_index = min(len(tokens), target_index + context_size + 1)\n",
        "        context_words = tokens[start_index:end_index]\n",
        "        context_sentence = ' '.join(context_words)\n",
        "        all_contexts.append(context_sentence)\n",
        "\n",
        "    return all_contexts\n",
        "\n"
      ],
      "metadata": {
        "id": "6BHWSytnyvJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample amazon scrape dataset\n",
        "df = pd.read_csv(\"amazon_smart_cameras_products_dataset.csv\")"
      ],
      "metadata": {
        "id": "gNvOWwCTwJFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_case_insensitive(text, target):\n",
        "  search = re.search(re.escape(target), text, re.IGNORECASE)\n",
        "\n",
        "  return search.span()[-1] - len(target) if search else -1"
      ],
      "metadata": {
        "id": "XNNEO8DhwJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we found out some common name in decription, if below words are there in context window, we tag the word as APP in NER label dataset.\n",
        "common_apps = ['Smart Life', 'Tuya', 'Kasa', 'Wansview', 'bn-link', 'WESECUU', 'AlfredCamera', 'Alfred', 'wyze', 'EOJO', 'meross', 'broadlink', 'wiser home', 'alexa', 'google home', 'smartlife' ]\n",
        "\n",
        "data = {}"
      ],
      "metadata": {
        "id": "6o0zInB02KXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df[df['long_description'].notna()][['brand', 'long_description']]\n",
        "\n",
        "for product in new_df.itertuples(index=False):\n",
        "  contexts = get_all_contexts(product.long_description, 'app')\n",
        "\n",
        "  for context in contexts:\n",
        "    temp_apps = [product.brand] + common_apps if len(product.brand)>2 else common_apps\n",
        "    for app in temp_apps:\n",
        "      brand_index = find_case_insensitive(context, app)\n",
        "\n",
        "      if brand_index !=-1:\n",
        "        #brand present\n",
        "        print(app, context)\n",
        "        data[context] = {\"entities\": [(brand_index, brand_index+ len(app)-1, \"APP\")]}\n",
        "        break\n",
        "    else:\n",
        "      index_app = find_case_insensitive(context, 'app')\n",
        "      data[context] = {\"entities\": [(0, index_app-2, \"APP\")]}\n"
      ],
      "metadata": {
        "id": "xCAc_6GNAL0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Label Dataset"
      ],
      "metadata": {
        "id": "dAOtYgMxXELM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for product in df.itertuples(index=False):\n",
        "    print(product)\n",
        "    for text in [product.long_description, product.short_description]:\n",
        "        contexts = get_all_contexts(text, 'app')\n",
        "\n",
        "        for context in contexts:\n",
        "            temp_apps = [product.brand] + common_apps if len(product.brand)>2 else common_apps\n",
        "            for app in temp_apps:\n",
        "                brand_index = find_case_insensitive(context, app)\n",
        "\n",
        "                if brand_index !=-1:\n",
        "                    #brand present\n",
        "                    print(app, context)\n",
        "                    data[context] = {\"entities\": [(brand_index, brand_index+ len(app)-1, \"APP\")]}\n",
        "                    break\n",
        "            else:\n",
        "                index_app = find_case_insensitive(context, 'app')\n",
        "                data[context] = {\"entities\": [(0, index_app-2, \"APP\")]}\n"
      ],
      "metadata": {
        "id": "4vViF419XFwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_dict(data, orient='index').reset_index()"
      ],
      "metadata": {
        "id": "jJccddPEZ_vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now save the dataset and manually label them"
      ],
      "metadata": {
        "id": "7tuRihVZ5c02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"amazon_camera_product_tag.json\", 'w') as f:\n",
        "  f.write(json.dumps(data))"
      ],
      "metadata": {
        "id": "OwIJg_hZ6HhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"amazon_product_tag_dataset.json\", \"r\") as f:\n",
        "  dataset = json.load(f)"
      ],
      "metadata": {
        "id": "m2dWnxCnVFFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "9ydYeVfvb9dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "XAur9j5NNDf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "from spacy.util import filter_spans\n",
        "from spacy.tokens import Doc, Span\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "nlp = spacy.blank('en')"
      ],
      "metadata": {
        "id": "Zu7J0NnoiWQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"amazon_product_tag_dataset.json\") as f:\n",
        "  dataset = json.load(f)"
      ],
      "metadata": {
        "id": "lDAB1KGiJ81-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelled_data = list(dataset.items())"
      ],
      "metadata": {
        "id": "5xT_bDENjHrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(labelled_data, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "l_sLR2GlSUO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_doc(nlp, data, output=\"data.spacy\"):\n",
        "  doc_bin = DocBin()\n",
        "  for example in tqdm(data):\n",
        "      text = example[0]\n",
        "      labels = example[1]['entities']\n",
        "      doc = nlp.make_doc(text)\n",
        "      ents = []\n",
        "\n",
        "      for start, end, label in labels:\n",
        "          span = doc.char_span(start, end+1, label=label, alignment_mode=\"contract\")\n",
        "          if span is None:\n",
        "              print(\"\\n\" + text)\n",
        "              print(\"Skipping entity:\", text[start:end+1], \"Start:\", start, \"End:\", end)\n",
        "          else:\n",
        "              ents.append(span)\n",
        "      filtered_ents = filter_spans(ents)\n",
        "      doc.ents = filtered_ents\n",
        "      doc_bin.add(doc)\n",
        "\n",
        "  doc_bin.to_disk(output)"
      ],
      "metadata": {
        "id": "jtyDfRJv-kN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_doc(nlp, train_data, \"train.spacy\")\n",
        "convert_to_doc(nlp, test_data, \"test.spacy\")"
      ],
      "metadata": {
        "id": "DpVdkub0K4f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile base_config.cfg\n",
        "# This is an auto-generated partial config. To use it with 'spacy train'\n",
        "# you can run spacy init fill-config to auto-fill all default settings:\n",
        "# python -m spacy init fill-config ./base_config.cfg ./config.cfg\n",
        "[paths]\n",
        "train = null\n",
        "dev = null\n",
        "vectors = \"en_core_web_lg\"\n",
        "[system]\n",
        "gpu_allocator = null\n",
        "\n",
        "[nlp]\n",
        "lang = \"en\"\n",
        "pipeline = [\"tok2vec\",\"ner\"]\n",
        "batch_size = 1000\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.tok2vec]\n",
        "factory = \"tok2vec\"\n",
        "\n",
        "[components.tok2vec.model]\n",
        "@architectures = \"spacy.Tok2Vec.v2\"\n",
        "\n",
        "[components.tok2vec.model.embed]\n",
        "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "attrs = [\"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"]\n",
        "rows = [5000, 1000, 2500, 2500]\n",
        "include_static_vectors = true\n",
        "\n",
        "[components.tok2vec.model.encode]\n",
        "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
        "width = 256\n",
        "depth = 8\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "\n",
        "[components.ner.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"ner\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 64\n",
        "maxout_pieces = 2\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.ner.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}"
      ],
      "metadata": {
        "id": "0jifAB4E89j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "id": "Jm__VDaXiWaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output case_insensitive_model --paths.train train.spacy --paths.dev test.spacy\n"
      ],
      "metadata": {
        "id": "sRTGW3o2iWcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m spacy evaluate model-best config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./train.spacy\n",
        "\n",
        "!python -m spacy evaluate case_insensitive_model/model-best test.spacy"
      ],
      "metadata": {
        "id": "Mdl8mt7u-NBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Mode on HuggingFace"
      ],
      "metadata": {
        "id": "RycxaXj8p8aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"case_sensitive_ner\"\n",
        "model_path_wheel = \"case_sensitive_ner_wheel\""
      ],
      "metadata": {
        "id": "w6_oJiPuq_35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##create output wheel path\n",
        "!mkdir $model_path_wheel"
      ],
      "metadata": {
        "id": "QM42PemmrjHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy package $model_path $model_path_wheel --build wheel"
      ],
      "metadata": {
        "id": "IJNxOloqruJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-huggingface-hub"
      ],
      "metadata": {
        "id": "uhnCqpMbsTNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "Cju1_AgLsaRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = !find $model_path_wheel -type f -name '*.whl'"
      ],
      "metadata": {
        "id": "0UKHnNUUtQJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = output_file[0]\n",
        "output_file"
      ],
      "metadata": {
        "id": "8I9bZ6mctirl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy huggingface-hub push $output_file"
      ],
      "metadata": {
        "id": "Gp8JyMcWs0kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://huggingface.co/MoinKhan3012/en_ner_sensitive_spacy/resolve/main/en_ner_sensitive_spacy-any-py3-none-any.whl"
      ],
      "metadata": {
        "id": "8l4hTkqPw1is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZFC0uvy7p-9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_ner = spacy.load('en_ner_sensitive_spacy')"
      ],
      "metadata": {
        "id": "2lt0whoBnflL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for td in test_data:\n",
        "  print(nlp_ner(td[0]))\n"
      ],
      "metadata": {
        "id": "-tVoX7cgpRFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_ner = spacy.load(r'case_insensitive_model/model-best')\n",
        "doc_bin = DocBin().from_disk(\"test.spacy\")\n",
        "docs = list(nlp_ner.pipe([doc.text for doc in doc_bin.get_docs(nlp_ner.vocab)]))\n",
        "spacy.displacy.render(docs, style=\"ent\", jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "id": "90OMXDMj_6t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform NER on text"
      ],
      "metadata": {
        "id": "swqF2KO-_q-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nlp_ner = spacy.load(r'case_insensitive_model/model-best')\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "\n",
        "nlp_ner = spacy.load('en_ner_sensitive_spacy')\n",
        "\n",
        "\n",
        "text = \"\"\"The Linksys App makes it simple to setup\"\"\"\n",
        "# text = \"alexa and google assistant easy-to-use app versatile ultrapro  app available\"\n",
        "contexts = get_all_contexts(text, \"app\")\n",
        "\n",
        "# contexts = [\"Control the outdoor smart plug from anywhere anytime. Work with Smart Life, Tuya, Minoston APP. Works with Amazon Alexa, Google Assistant. Simply use your voice command to control your home devices.\"]\n",
        "print(contexts)\n",
        "result = []\n",
        "app_name = defaultdict(int)\n",
        "\n",
        "\n",
        "app_cnt = []\n",
        "for context in contexts:\n",
        "  doc = nlp_ner(context)\n",
        "\n",
        "\n",
        "  # Print entities in the processed document\n",
        "  for ent in doc.ents:\n",
        "      name = re.sub(\"[^a-zA-Z0-9]\", \"\", ent.text.upper())\n",
        "      app_name[name]  +=1\n",
        "\n",
        "\n",
        "max_count = max(app_name.values())\n",
        "max_keys = [(key, count) for key, count in app_name.items() if count == max_count]\n",
        "max_keys"
      ],
      "metadata": {
        "id": "17auBWevpRop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =  \"\"\"Simply use the free Smartlife App to control your devices\"\"\"\n",
        "nlp_ner = spacy.load('case_insensitive_model/model-best')\n",
        "\n",
        "doc = nlp_ner(text)\n",
        "\n",
        "# Print entities in the processed document\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "WytGvXzjiWXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT NER MODEL"
      ],
      "metadata": {
        "id": "_T6-j414XS8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets seqeval evaluate\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install  transformers==4.30"
      ],
      "metadata": {
        "id": "r8x2A_zxXgEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "kYdiAOEawRrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_dataset = json.load(open('amazon_product_tag.json'))"
      ],
      "metadata": {
        "id": "YfbCjXd889pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_dict(tag_dataset, orient='index').reset_index()\n",
        "# pd.DataFrame(list(dataset_json.items()), columns=['index', 'entities'])\n",
        "\n"
      ],
      "metadata": {
        "id": "VtSBMEYvsXhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tag Dataset"
      ],
      "metadata": {
        "id": "WjnSpxaUXofz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the tag dataset from JSON format to BIO (Begin-Inside-Outside) format as required by the BERT model."
      ],
      "metadata": {
        "id": "wh_EDhUXmBKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = pd.DataFrame(columns=['tokens', 'ner_tags'])\n",
        "for key, value in tag_dataset.items():\n",
        "    text = key\n",
        "    tokens = text.split()\n",
        "    labels = ['O'] * len(tokens)\n",
        "    for start, end, tag in value['entities']:\n",
        "        label_start_index = text[:start].count(\" \")\n",
        "        labels[label_start_index] = \"B-APP\"\n",
        "        #check if there are any spaces in the labelled app\n",
        "        spaces = text[start: end+1].count(\" \")\n",
        "        if spaces > 0:\n",
        "            #add Intermediate App label\n",
        "            for i in range(1, spaces+1):\n",
        "                labels[label_start_index+i] = 'I-APP'\n",
        "\n",
        "        #add app word as I-APP\n",
        "        # if text[end+1: end+5].lower().strip()=='app':\n",
        "        #     labels[label_start_index + spaces + 1] = 'I-APP'\n",
        "\n",
        "    tokenized_data = pd.concat([tokenized_data, pd.DataFrame([{'tokens': tokens, 'text': text, 'ner_tags': labels}])], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "dW6TEdXb1Xc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_tokenize_data(tag_dataset_json):\n",
        "    tokenized_data = pd.DataFrame(columns=['tokens', 'ner_tags'])\n",
        "    for key, value in tag_dataset.items():\n",
        "        text = key\n",
        "        tokens = text.split()\n",
        "        labels = ['O'] * len(tokens)\n",
        "        for start, end, tag in value['entities']:\n",
        "            label_start_index = text[:start].count(\" \")\n",
        "            labels[label_start_index] = \"B-APP\"\n",
        "            #check if there are any spaces in the labelled app\n",
        "            spaces = text[start: end+1].count(\" \")\n",
        "            if spaces > 0:\n",
        "                #add Intermediate App label\n",
        "                for i in range(1, spaces+1):\n",
        "                    labels[label_start_index+i] = 'I-APP'\n",
        "\n",
        "        tokenized_data = pd.concat([tokenized_data, pd.DataFrame([{'tokens': tokens, 'text': text, 'ner_tags': labels}])], ignore_index=True)\n",
        "\n",
        "    return tokenized_data"
      ],
      "metadata": {
        "id": "qxSFiC3CD7tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = create_tokenize_data(tag_dataset)"
      ],
      "metadata": {
        "id": "WQNNSGHK2aXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile script.py\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "def create_tokenize_data(tag_dataset_json):\n",
        "    tokenized_data = pd.DataFrame(columns=['tokens', 'ner_tags'])\n",
        "    for key, value in tag_dataset.items():\n",
        "        text = key\n",
        "        tokens = text.split()\n",
        "        labels = ['O'] * len(tokens)\n",
        "        for start, end, tag in value['entities']:\n",
        "            label_start_index = text[:start].count(\" \")\n",
        "            labels[label_start_index] = \"B-APP\"\n",
        "            #check if there are any spaces in the labelled app\n",
        "            spaces = text[start: end+1].count(\" \")\n",
        "            if spaces > 0:\n",
        "                #add Intermediate App label\n",
        "                for i in range(1, spaces+1):\n",
        "                    labels[label_start_index+i] = 'I-APP'\n",
        "\n",
        "        tokenized_data = pd.concat([tokenized_data, pd.DataFrame([{'tokens': tokens, 'text': text, 'ner_tags': labels}])], ignore_index=True)\n",
        "\n",
        "    return tokenized_data\n",
        "\n",
        "def tokenize_adjust_labels(all_samples_per_split, tokenizer):\n",
        "\n",
        "    total_adjusted_labels = []\n",
        "    label_names = {'O': 0, 'B-APP': 1, 'I-APP': 2}\n",
        "\n",
        "    tokenized_samples = tokenizer(all_samples_per_split[\"text\"])\n",
        "\n",
        "    word_ids_list = tokenized_samples.word_ids()\n",
        "    existing_label_ids = [-100] + [label_names[tag] for tag in all_samples_per_split[\"ner_tags\"]] + [-100]\n",
        "\n",
        "    tokenized_samples['labels'] = existing_label_ids\n",
        "\n",
        "    return pd.Series(tokenized_samples)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    label_names = {\n",
        "        0: 'O', 1:'B-APP', 2: 'I-APP'\n",
        "    }\n",
        "    print(p)\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    print(predictions)\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    flattened_results = {\n",
        "        \"overall_precision\": results[\"overall_precision\"],\n",
        "        \"overall_recall\": results[\"overall_recall\"],\n",
        "        \"overall_f1\": results[\"overall_f1\"],\n",
        "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "    for k in results.keys():\n",
        "      if(k not in flattened_results.keys()):\n",
        "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
        "\n",
        "    return flattened_results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"extracting arguments\")\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
        "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
        "    parser.add_argument(\"--train_batch_size\", type=int, default=16)\n",
        "    parser.add_argument(\"--eval_batch_size\", type=int, default=16)\n",
        "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
        "    parser.add_argument(\"--model_name\", type=str)\n",
        "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
        "\n",
        "    # Data, model, and output directories\n",
        "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
        "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
        "    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\n",
        "    parser.add_argument(\"--data-file\", type=str)\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "\n",
        "    # load json data\n",
        "    data_df = json.load(args.data_file)\n",
        "\n",
        "    # create tokenized data - converts JSON format to BIO format for NER\n",
        "    tokenized_data = create_tokenize_data(tag_dataset)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    tokenized_data = tokenized_data.apply(lambda row: tokenize_adjust_labels(row, tokenizer), axis=1)\n",
        "\n",
        "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "    print(\"building training and testing datasets\")\n",
        "\n",
        "    # split dataset\n",
        "    train_df, test_df = train_test_split(tokenized_data, test_size=0.3, random_state=42)\n",
        "\n",
        "    # load dataset from pandas to HF\n",
        "    train_data = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "    test_data = Dataset.from_pandas(test_df, preserve_index=False)\n",
        "\n",
        "    # metric to monitor\n",
        "    metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "    id2label= {\n",
        "        \"0\": \"LABEL_0\",\n",
        "        \"1\": \"LABEL_1\",\n",
        "        \"2\": \"LABEL_2\"\n",
        "    }\n",
        "\n",
        "    label2id= {\n",
        "        \"LABEL_0\": \"0\",\n",
        "        \"LABEL_1\": \"1\",\n",
        "        \"LABEL_2\": \"2\"\n",
        "    }\n",
        "\n",
        "\n",
        "    #initialized base model\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", id2label=id2label, label2id=label2id)\n",
        "\n",
        "\n",
        "    # set training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.model_dir,\n",
        "        num_train_epochs=args.epochs,\n",
        "        per_device_train_batch_size=args.train_batch_size,\n",
        "        per_device_eval_batch_size=args.eval_batch_size,\n",
        "        warmup_steps=args.warmup_steps,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
        "        learning_rate=float(args.learning_rate),\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "\n",
        "    # initialized trainer job\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=test_data,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    # Persist model\n",
        "\n",
        "    # evaluate model\n",
        "    eval_result = trainer.evaluate(eval_dataset=test_data)\n",
        "\n",
        "    # writes eval result to file which can be accessed later in s3 ouput\n",
        "    with open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\") as writer:\n",
        "        print(f\"***** Eval results *****\")\n",
        "        for key, value in sorted(eval_result.items()):\n",
        "            writer.write(f\"{key} = {value}\\n\")\n",
        "\n",
        "    # Saves the model to s3\n",
        "    trainer.save_model(args.model_dir)\n",
        "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
        "    joblib.dump(model, path)\n",
        "    print(\"model persisted at \" + path)"
      ],
      "metadata": {
        "id": "np5CfipAF2aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize and adjust the labels"
      ],
      "metadata": {
        "id": "T6nimM-PXvHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_adjust_labels(all_samples_per_split, tokenizer):\n",
        "\n",
        "    total_adjusted_labels = []\n",
        "    label_names = {'O': 0, 'B-APP': 1, 'I-APP': 2}\n",
        "\n",
        "    tokenized_samples = tokenizer(all_samples_per_split[\"text\"])\n",
        "\n",
        "    word_ids_list = tokenized_samples.word_ids()\n",
        "    existing_label_ids = [-100] + [label_names[tag] for tag in all_samples_per_split[\"ner_tags\"]] + [-100]\n",
        "\n",
        "    tokenized_samples['labels'] = existing_label_ids\n",
        "\n",
        "    return pd.Series(tokenized_samples)\n"
      ],
      "metadata": {
        "id": "jUx8mZt4-ELy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenized_data = tokenized_data.apply(lambda row: tokenize_adjust_labels(row, tokenizer), axis=1)"
      ],
      "metadata": {
        "id": "dC3YV_xPra-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "oxhMQq9cX0g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "mXH8oUdcE2yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    label_names = {\n",
        "        0: 'O', 1:'B-APP', 2: 'I-APP'\n",
        "    }\n",
        "    print(p)\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    print(predictions)\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    flattened_results = {\n",
        "        \"overall_precision\": results[\"overall_precision\"],\n",
        "        \"overall_recall\": results[\"overall_recall\"],\n",
        "        \"overall_f1\": results[\"overall_f1\"],\n",
        "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "    for k in results.keys():\n",
        "      if(k not in flattened_results.keys()):\n",
        "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
        "\n",
        "    return flattened_results"
      ],
      "metadata": {
        "id": "mdf4su4gE3TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(tokenized_data, test_size=0.3, random_state=42)\n",
        "\n",
        "train_data = Dataset.from_pandas(train_data, preserve_index=False)\n",
        "test_data = Dataset.from_pandas(test_data, preserve_index=False)\n",
        "\n",
        "id2label= {\n",
        "    \"0\": \"LABEL_0\",\n",
        "    \"1\": \"LABEL_1\",\n",
        "    \"2\": \"LABEL_2\"\n",
        "}\n",
        "\n",
        "label2id= {\n",
        "    \"LABEL_0\": \"0\",\n",
        "    \"LABEL_1\": \"1\",\n",
        "    \"LABEL_2\": \"2\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "V1qRLJtPIQO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "NPYqzj1sSq49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", id2label=id2label, label2id=label2id)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./custom_ner_bert-uncased\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps = 100,\n",
        "    run_name = \"ep_10_tokenized_11\",\n",
        "    save_strategy='no',\n",
        "    remove_unused_columns=False\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ucf6LNZKFaEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "5Eh80VPgX9aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.save_model(\"custom_ner_bert-uncased\")\n",
        "model.save_pretrained(\"custom_ner_bert-uncased\")"
      ],
      "metadata": {
        "id": "IT1s3sJXRGmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "TsSy_CSPBAHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"moinkhan3012/custom_ner_bert-uncased\")"
      ],
      "metadata": {
        "id": "pUbPJxkPRf47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Inference"
      ],
      "metadata": {
        "id": "SfUx9_xhYA-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "from transformers import AutoModelForTokenClassification\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "loaded_model = AutoModelForTokenClassification.from_pretrained(\"custom_ner_bert\", num_labels=3)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"custom_ner_bert\")\n",
        "nlp = pipeline(\"token-classification\", model=loaded_model, tokenizer=tokenizer)\n",
        "\n",
        "label_mapping = {\n",
        "    \"LABEL_0\": \"O\",\n",
        "    \"LABEL_1\": \"B-APP\",\n",
        "    \"LABEL_2\": \"I-APP\"\n",
        "}"
      ],
      "metadata": {
        "id": "QXM9M75YVKrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_tokens(predictions, label_mapping):\n",
        "    reconstructed_tokens = []\n",
        "    current_word = []\n",
        "    current_label = None\n",
        "\n",
        "    for token in predictions:\n",
        "        if token['word'].startswith('##'):  # Handle subwords\n",
        "            current_word.append(token['word'][2:])  # Remove '##' prefix\n",
        "        else:\n",
        "            if current_word:  # If there's a current word, it's complete\n",
        "                reconstructed_word = ''.join(current_word)\n",
        "                reconstructed_tokens.append({\n",
        "                    'word': reconstructed_word,\n",
        "                    'entity': label_mapping[current_label]\n",
        "                })\n",
        "                current_word = []\n",
        "                current_label = None\n",
        "\n",
        "            current_word.append(token['word'])\n",
        "            current_label = token['entity']\n",
        "\n",
        "    # Handle the last word if any\n",
        "    if current_word:\n",
        "        reconstructed_word = ''.join(current_word)\n",
        "        reconstructed_tokens.append({\n",
        "            'word': reconstructed_word,\n",
        "            'entity': label_mapping[current_label]\n",
        "        })\n",
        "\n",
        "    print(reconstructed_tokens)\n",
        "\n",
        "    app_name = []\n",
        "    for entity in reconstructed_tokens:\n",
        "        if entity['entity']=='B-APP':\n",
        "            app_name.append(entity['word'])\n",
        "        elif app_name and entity['entity']=='I-APP':\n",
        "            app_name[-1] += f\" {entity['word']}\"\n",
        "\n",
        "    return app_name\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZbsPUIT_IhWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"amazon_smart_cameras_products_dataset.csv\")"
      ],
      "metadata": {
        "id": "Rk3ARG8-o8HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "QR7AepZYcHHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "D8aMWI0RmsUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "def get_all_contexts(text, target_word, context_size=5):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(re.sub('[a-zA-Z0-9]+', '', text))\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "    # print(tokens)\n",
        "    # Find all occurrences of the target word\n",
        "\n",
        "    target_indices = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        match  = re.findall('[a-zA-Z0-9]+', token)\n",
        "        if match and match[0].lower() == target_word.lower():\n",
        "            target_indices.append(i)\n",
        "\n",
        "    # Extract context sentences for each occurrence of the target word\n",
        "    all_contexts = []\n",
        "    for target_index in target_indices:\n",
        "        start_index = max(0, target_index - context_size)\n",
        "        end_index = min(len(tokens), target_index + context_size + 1)\n",
        "        context_words = tokens[start_index:end_index]\n",
        "        context_sentence = ' '.join(context_words)\n",
        "        all_contexts.append(context_sentence)\n",
        "\n",
        "    return all_contexts\n"
      ],
      "metadata": {
        "id": "0r_UeYrbb9XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "def normalize_string(s):\n",
        "    return s.lower().replace(' ', '')\n",
        "\n",
        "def are_similar(s1, s2, threshold=80):\n",
        "    return fuzz.token_sort_ratio(normalize_string(s1), normalize_string(s2)) >= threshold\n",
        "\n",
        "def group_similar_strings(strings, threshold=50):\n",
        "    groups = []\n",
        "    for string in strings:\n",
        "        matched = False\n",
        "        for group in groups:\n",
        "            if any(are_similar(string, existing_str, threshold) for existing_str in group):\n",
        "                group.append(string)\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            groups.append([string])\n",
        "    return groups"
      ],
      "metadata": {
        "id": "ANjhivngnrgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for index, row in df.iterrows():\n",
        "    app_names = []\n",
        "    for text in [row['short_description'], row['long_description']]:\n",
        "\n",
        "        contexts = get_all_contexts(text, 'app')\n",
        "        print(contexts)\n",
        "        for context in contexts:\n",
        "            ner_results = nlp(re.sub('[^a-zA-Z0-9 ]+', '', context))\n",
        "            app_names.extend(reconstruct_tokens(ner_results, label_mapping))\n",
        "\n",
        "    if app_names:\n",
        "        app_names  = group_similar_strings(app_names, threshold=50)\n",
        "        print(index)\n",
        "        # df.loc[index, 'APP_NAME'] = sorted(app_names, key=len, reverse=True)[0][0]\n",
        "        print(df.loc[index, 'APP_NAME'])"
      ],
      "metadata": {
        "id": "jXWXRx8vq8Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"amazon_smart_cameras_products_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "7W72IFU0_5p8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}